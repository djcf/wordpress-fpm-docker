## Overview
This document describes the lifecycle of HTTP requests when those requests are destined for a vhost set to use one of the on-demand containers.

** Please read `A Note on System Identifiers.md`, first.**

## To open a shell

* Start the container: `docker start $domain.fpm`
* Open a shell: `docker exec -it $domain.fpm ash`

Problems using nano? (Error: "TERM is undefined") Type:

    export TERM=xterm

### In the begining: nginx

First, the URI is matched according to one of three main location directives.

If the directive matches `wp-content`, it **must** refer to a static asset. It is loaded **directly** from `/var/www/$domain/public_html/wp-content` and php execution is disabled for this request.

If the directive matches a non-wp-content **static asset**, it is loaded directly from `/usr/src/wordpress`. Php is not invoked.

Finally, if it matches a php script in /usr/src/wordpress, it is sent to a socket listening in /var/run/docker-apps/$domain/vhost-waker.sock

However, that socket does not belong to fpm. It belongs to *the host's* systemd.

### On-demand socket activation

When the container is first instantiated, ansible runs the following commands in addition to the `docker create ...` command.

	systemctl enable fpm-waker@$domain.socket
	systemctl start fpm-waker@$domain.socket

These commands activate systemd service files, which are defined in `/etc/systemd/system`.

$domain is replaced with the full subdomain ID used to identify the container, such as a container named `test.fpm` for the subdomain test.common.scot. For example, `systemctl start fpm-waker@test.socket`.

When the server's systemd receives this command, it reads the template file describing the socket in `/etc/systemd/system/vhost-waker@.socket`. That tells it to create a unix listening socket in /var/run/docker-apps/$domain/vhost-waker.sock.

Then, it waits.

**When nginx receives a request for that container, it sends the request to the socket which was created earlier by systemd.**

When the socket is activated, systemd itself runs this command:

	systemctl start fpm-waker@$domain.service
	
This command tells systemd to do two things. Firstly, to run this command:

	systemctl start fpm@$domain.service

And secondly to start a proxy-program which will hold nginx's request until fpm@$domain.service has finished starting.

The systemd service `fpm@$domain.service` itself has a very simply unit declaration. Essentially, `fpm@$domain.service` is itself programmed to run this command:

	docker start $domain.fpm
	
Finally, within the docker container named `$domain.fpm`, when the php-fpm daemon has finished starting, the fpm daemon creates a listening socket which is the final destination for the request: `/var/run/docker-apps/$domain/php-fpm.sock`.

** To recap:** The control flow goes like this:

-2. Ansible: systemctl start fpm-waker@$domain.sock

-1. systemd creates /var/run/docker-apps/$domain/vhost-waker.sock

**Later, a request is received by nginx**:

1. nginx.conf -> Matches with $domain vhost nginx config
2. $domain vhost config -> Sends request to /var/run/docker-apps/$domain/vhost-waker.sock
3. systemd (listening to vhost-waker.sock) -> Executes systemctl start fpm-waker@$domain.service
4. fpm-waker@$domain.service:
	-> Executes /usr/local/bin/socket-proxy.sh (waits for php to be ready)
	-> Executes systemctl start fpm@$domain.service
5. fpm@$domain.service -> executes systemd-docker start $domain.fpm
	-> systemd-docker executes docker start $domain.fpm (see When things go wrong section for why this is nessessary)
6. $domain.fpm container -> Creates /var/run/docker-apps/$domain/php-fpm.sock
7. socket-proxy.sh -> Senses activation of php-fpm.sock, and connects nginx to it.
	
And the whole process works surprisingly well.

### For more information about systemd service and sockets

Please read this next: https://labs.common.scot/CommonWeal/web-two-point-oh/src/master/ansible/roles/install-web/files/systemd.services/README.md

## Shutting down a container

All containers log their requests to stdout, where they are collected by docker daemon for use by `docker logs`. `docker logs` can further be filtered to only show logs from the last XX minutes.

It's quite a simple process for a cron script, `/usr/local/cron/pause-on-demand-containers.sh`, to execute `docker logs --since XXm` and pause containers which have no output.

That script needs only two things: a list of which containers to pause, and an amount of time to filter `docker logs` by. It gets the first from the directory listing of /var/lib/docker/volumes/docker_sites-enabled/on-demand/*.conf. It gets the second from extracting the value after the first colon (:) in the first line of the file, e.g., from parsing line 1:

	# Amount of time to pause the container after: 20m
	
It then runs `systemctl stop fpm@$domain.service`, which as you may guess, itself runs `docker stop $domain.fpm`.

### When things go wrong

The `docker` system binary is a **client**. It *talks* to the docker daemon, then quits. As a result, when systemd systemctl runs `docker start/stop`, to talk to the docker daemon, it isn't guaranteed that systemctl is kept up to date with what happens just by looking at the return status of the docker client.

For example if you ran `docker start/stop $domain.fpm` yourself, systemd systemctl would not be aware you had done so. It would then get confused, thinking the container (and its associated sockets etc.) were active when they were not or vice versa.

To fix this problem, we use https://github.com/ibuildthecloud/systemd-docker. It's a simple proxy which lets systemd be kept in touch with container activity. Without it, `service $name status` would always show the status of the docker client binary, and not the status of the container. That's why the actual unit files read `ExecStart=/usr/binb/systemd-docker start $container`, instead of `docker start container` as you might have been expecting from the section above.

A sympton of this problem is that Nginx will report a 500 error, and no sockets or the wrong sockets are visible in /var/run/docker-apps/$domain.

It's actually quite a simple process to resolve, just run:

	systemctl stop fpm-waker@$domain.sock
	systemctl stop fpm-waker@$domain.service
	systemctl stop fpm@$domain.service
	systemctl start fpm-waker@$domain.sock

Actually, **dont run that**. Just run `/usr/local/bin/renew-fpm-socket.sh $domain`, instead.

### Further Reading

* http://www.dest-unreach.org/socat/doc/README
* http://danwalsh.livejournal.com/74421.html
* https://torusware.com/blog/2015/04/optimizing-communications-between-html/
